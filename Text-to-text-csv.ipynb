{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akovognon2003/Arduino_JSON/blob/master/Text-to-text-csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 Fine-Tuning Tutorial with PyTorch & Huggingface in **Colab**"
      ],
      "metadata": {
        "id": "b6rh7uabUw_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il s'agit d'un script simplifié permettant d'affiner GPT2 en utilisant la bibliothèque Transformers de Hugging Face et PyTorch.\n",
        "\n",
        "Vous devez comprendre les bases de PyTorch et le fonctionnement d'une boucle d'entraînement avant de commencer. La connaissance du fonctionnement de GPT2 peut être utile mais n'est pas nécessaire. Le code a été écrit dans un souci de clarté et non de réutilisation. Je conseille de le remanier pour des projets réels. J'ai généreusement repris des éléments du tutoriel de Chris McCormick sur le réglage fin du BERT, du tutoriel de Ian Porter sur le GPT2 et du script de réglage fin du modèle Hugging Face Language, donc tout le mérite leur revient. Le code de Chris a pratiquement fourni la base de ce script - vous devriez absolument consulter son blog.\n",
        "\n",
        "Je dois mentionner ce que le script ne couvre pas :\n",
        "\n",
        "  - L'utilisation de la bibliothèque nlp pour charger l'ensemble de données et mettre en place le flux de travail d'entraînement, qui semble rationaliser les choses de manière assez agréable.\n",
        "  - Accumulation de gradients - cela permet d'obtenir des tailles de lots effectives plus importantes que celles autorisées par Colab (GPT2 est un modèle de grande taille, et toute taille de lot supérieure à 2 suffirait à provoquer une erreur CUDA de mémoire insuffisante sur Colab).\n",
        "  - Gel des couches. Il s'agit du processus consistant à ne modifier que les paramètres des couches sélectionnées, rendu célèbre par le processus ULMFit.\n",
        "  - Utilisation de \"past\" lors de la génération de texte. Cela permet de prendre en compte l'état précédent lors de la génération d'éléments de texte successifs. Je n'en ai pas eu besoin.\n",
        "  - Emballage des tenseurs. Il s'agit d'un moyen efficace d'intégrer un maximum de données d'entraînement dans chaque lot.\n",
        "  - Recherche d'hyperparamètres. J'ai rapidement choisi des valeurs qui semblaient produire des valeurs correctes, sans vérifier si elles étaient optimales.\n",
        "\n",
        "Nous avons réutilisé ainsi ce tutoriel dans le cadre de la formation sur l'EEIA 2023. Ainsi, nous ne partons pas de zéro et orientons mieux les participants sur le fine turning d'un modele Text to Text avec GPT2.\n",
        "Vous pouvez revoir le code complet à cet adresse https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh?usp=sharing#scrollTo=EKOTlwcmxmej ."
      ],
      "metadata": {
        "id": "d-SqOsWzU_ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Il est important de souligner que ce travail se base sur un travail initiale où nous avons adapté le modele gpt2 aux données du covid19\n",
        "\n",
        "__author__ = \"NOBIME Tanguy Adonis\"\n",
        "\n",
        "__copyright__ = \"Copyright 2023, EEIA 2023\"\n",
        "\n",
        "__credits__ = [\"Vincent Whannou de Dravo, Vivien Ogoun, Adonis Nobime, Landry Bossou and Konig Koudogbo\"]\n",
        "\n",
        "__license__ = \"MIT\"\n",
        "\n",
        "__version__ = \"1.0.0\"\n",
        "\n",
        "__maintainer__ = \"Vivien Ogoun, and Vincent Whannou de Dravo and Adonis Nobime and Landry Bossou and Konig Koudogbo\"\n",
        "\n",
        "__email__ = \"nobimetanguy19@gmail.com\"\n",
        "\n",
        "__status__ = \"Dev\"\n",
        "\n"
      ],
      "metadata": {
        "id": "eSxxllZ9VRCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation du package transformers avec cette commande :\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "pNnkLibkVSu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "n6dHl79mS-vM",
        "outputId": "59d8720a-3769-4b9e-b97c-d591d0be5e06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importation des packages nécessaires"
      ],
      "metadata": {
        "id": "81bqsJ71Vev1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
        "torch.manual_seed(42)\n",
        "\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1FrRgiFBS-zx",
        "outputId": "344843f3-320d-4cc8-c5c7-88e181345e19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Création d'un ensemble d'entraînement\n",
        "\n",
        "Les données utilisées pour affiner le modèle linguistique sont un ensemble d'environ 1000 ensemble de données COVID sur la détection des fausses nouvelles, dans le but de les générer dans le même format général et le même style."
      ],
      "metadata": {
        "id": "GSr4HFroVm3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/covid_faq.csv\""
      ],
      "metadata": {
        "id": "LqqLMUaaS-5N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lecture des données\n",
        "df = pd.read_csv(path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "AgtoNyV4S-_K",
        "outputId": "495840bd-2a73-41c3-c39c-88737aff6935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           questions  \\\n",
              "0                       What is a novel coronavirus?   \n",
              "1  Why is the disease being called coronavirus di...   \n",
              "2                         How does the virus spread?   \n",
              "3  Can I get COVID-19 from food (including restau...   \n",
              "4   Will warm weather stop the outbreak of COVID-19?   \n",
              "\n",
              "                                             answers  \n",
              "0  A novel coronavirus is a new coronavirus that ...  \n",
              "1  On February 11, 2020 the World Health Organiza...  \n",
              "2  The virus that causes COVID-19 is thought to s...  \n",
              "3  Currently there is no evidence that people can...  \n",
              "4  It is not yet known whether weather and temper...  "
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-f5fa6a69-4f8e-4ae8-b026-761d8550e795\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>questions</th>\n",
              "      <th>answers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is a novel coronavirus?</td>\n",
              "      <td>A novel coronavirus is a new coronavirus that ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Why is the disease being called coronavirus di...</td>\n",
              "      <td>On February 11, 2020 the World Health Organiza...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How does the virus spread?</td>\n",
              "      <td>The virus that causes COVID-19 is thought to s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Can I get COVID-19 from food (including restau...</td>\n",
              "      <td>Currently there is no evidence that people can...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Will warm weather stop the outbreak of COVID-19?</td>\n",
              "      <td>It is not yet known whether weather and temper...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5fa6a69-4f8e-4ae8-b026-761d8550e795')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-42a9d164-ead7-45dd-a31b-dae632e4a319\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-42a9d164-ead7-45dd-a31b-dae632e4a319')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-42a9d164-ead7-45dd-a31b-dae632e4a319 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f5fa6a69-4f8e-4ae8-b026-761d8550e795 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f5fa6a69-4f8e-4ae8-b026-761d8550e795');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse des données manquantes\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "1lxu9aiES_E-",
        "outputId": "d822da83-1c51-43aa-cde1-b3ef0a3584bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "questions    0\n",
              "answers      0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prise des réponses sur la covid\n",
        "answers_covid = df.answers.copy()"
      ],
      "metadata": {
        "id": "nj6-12iIS_Ky"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous devons nous faire une idée de la longueur de nos documents d'apprentissage.\n",
        "\n",
        "Je ne vais pas utiliser le même tokenizer que celui de GPT2, qui est un tokenizer de codage par paire d'octets. Au lieu de cela, j'en utilise un simple, juste pour avoir une idée approximative."
      ],
      "metadata": {
        "id": "HX5KEh3qVypa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_lengths = []\n",
        "\n",
        "for bio in answers_covid :\n",
        "\n",
        "    # obtenir une distribution approximative du nombre de jetons\n",
        "    tokens = nltk.word_tokenize(bio)\n",
        "\n",
        "    doc_lengths.append(len(tokens))\n",
        "\n",
        "doc_lengths = np.array(doc_lengths)\n",
        "\n",
        "sns.distplot(doc_lengths)"
      ],
      "metadata": {
        "id": "iYxqPeWMS_Rq",
        "outputId": "e6534758-2330-41d9-8b92-48882f6bf3b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-889de062fe13>:12: UserWarning: \n",
            "\n",
            "`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n",
            "\n",
            "Please adapt your code to use either `displot` (a figure-level function with\n",
            "similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "\n",
            "For a guide to updating your code to use the new functions, please see\n",
            "https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n",
            "\n",
            "  sns.distplot(doc_lengths)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='Density'>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help"
      ],
      "metadata": {
        "id": "YzVqVf9zWIqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# la longueur moyenne d'un jeton\n",
        "np.average(doc_lengths)"
      ],
      "metadata": {
        "id": "oyHlYF_xS_Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT2 Tokenizer\n",
        "Bien que les valeurs par défaut s'en chargent, j'ai pensé montrer qu'il est possible de spécifier certains des jetons spéciaux."
      ],
      "metadata": {
        "id": "qmv-CIu1V_qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lecture de Tokenizer.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')"
      ],
      "metadata": {
        "id": "ZcdZ4GglS_en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensembles de données et chargeurs de données PyTorch\n",
        "GPT2 est un modèle de grande taille. L'augmentation de la taille des lots au-delà de 2 a conduit à des problèmes de mémoire insuffisante. Ce problème peut être atténué en accumulant les gradients, mais cela n'entre pas dans le cadre de cette étude."
      ],
      "metadata": {
        "id": "rOAirQ5SWQBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2"
      ],
      "metadata": {
        "id": "SDblGflTWUHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'utilise l'approche standard de PyTorch pour charger les données en utilisant une classe dataset.\n",
        "\n",
        "Je passe le tokenizer en argument mais normalement je devrais l'instancier dans la classe."
      ],
      "metadata": {
        "id": "ocihGNN7WVJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Dataset(Dataset):\n",
        "\n",
        "  def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=768):\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.attn_masks = []\n",
        "\n",
        "    for txt in txt_list:\n",
        "\n",
        "      encodings_dict = tokenizer('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "\n",
        "      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.attn_masks[idx]"
      ],
      "metadata": {
        "id": "Vvv_h6FFWcDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour comprendre comment j'ai utilisé le tokenizer, il est utile de lire la documentation. J'ai enveloppé chaque bio dans les jetons bos et eos.\n",
        "\n",
        "Chaque tenseur transmis au modèle doit avoir la même longueur.\n",
        "\n",
        "Si la bio est plus courte que 768 tokens, elle sera ramenée à une longueur de 768 en utilisant le token padding. De plus, un masque d'attention sera renvoyé et devra être transmis au modèle pour lui indiquer d'ignorer les jetons de remplissage.\n",
        "\n",
        "Si la bio est plus longue que 768 tokens, elle sera tronquée sans le eos_token. Ce n'est pas un problème."
      ],
      "metadata": {
        "id": "0vSoUrGaWgzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = GPT2Dataset(answers_covid, tokenizer, max_length=768)\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "metadata": {
        "id": "LQYBw2JMWqNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer les DataLoaders pour nos ensembles de données d'entraînement et de validation.\n",
        "# Nous allons prendre des échantillons d'entraînement dans un ordre aléatoire.\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # Les échantillons de formation.\n",
        "            sampler = RandomSampler(train_dataset), # Sélectionner les lots de manière aléatoir\n",
        "            batch_size = batch_size # Trains avec cette taille de lot.\n",
        "        )\n",
        "\n",
        "# Pour la validation, l'ordre n'a pas d'importance, nous les lirons donc de manière séquentielle.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # Les échantillons de validation.\n",
        "            sampler = SequentialSampler(val_dataset), # Retirer les lots de manière séquentielle.\n",
        "            batch_size = batch_size # Évaluer avec cette taille de lot.\n",
        ")"
      ],
      "metadata": {
        "id": "MTRpLjZ-WvDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune GPT2 Language Model"
      ],
      "metadata": {
        "id": "V_CFp4rNWz2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Je ne fais rien avec la configuration buheret\n",
        "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
        "\n",
        "# instanciation du modèle\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
        "\n",
        "# cette étape est nécessaire car j'ai ajouté des tokens (bos_token, etc) aux embeddings\n",
        "# sinon les tenseurs du tokenizer et du modèle ne correspondront pas\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Indiquer à pytorch d'exécuter ce modèle sur le GPU.\n",
        "device = torch.device(\"cuda\")\n",
        "model.cuda()\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "2vsCllOmW5nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quelques paramètres que j'ai élaborés et qui fonctionnent raisonnablement bien\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 5e-4\n",
        "warmup_steps = 1e2\n",
        "epsilon = 1e-8\n",
        "\n",
        "# ceci produit un échantillon de sortie tous les 100 pas\n",
        "sample_every = 100"
      ],
      "metadata": {
        "id": "Iv_57SFHW_pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note : AdamW est une classe de la bibliothèque huggingface (par opposition à pytorch)\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate,\n",
        "                  eps = epsilon\n",
        "                )"
      ],
      "metadata": {
        "id": "7KsE6ST8XBjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Le nombre total d'étapes d'apprentissage est [nombre de lots] x [nombre d'époques].\n",
        "# (Notez que ce n'est pas la même chose que le nombre d'échantillons d'apprentissage).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Créer le planificateur du taux d'apprentissage.\n",
        "# Il modifie le taux d'apprentissage au fur et à mesure de la progression de la boucle d'apprentissage.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = warmup_steps,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "gcGGFLUVXEhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(elapsed):\n",
        "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
      ],
      "metadata": {
        "id": "FXJwIWyEXNsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_t0 = time.time()\n",
        "\n",
        "training_stats = []\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    total_train_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        outputs = model(  b_input_ids,\n",
        "                          labels=b_labels,\n",
        "                          attention_mask = b_masks,\n",
        "                          token_type_ids=None\n",
        "                        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_train_loss += batch_loss\n",
        "\n",
        "        # Get sample every x batches.\n",
        "        if step % sample_every == 0 and not step == 0:\n",
        "\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            sample_outputs = model.generate(\n",
        "                                    bos_token_id=random.randint(1,30000),\n",
        "                                    do_sample=True,\n",
        "                                    top_k=50,\n",
        "                                    max_length = 200,\n",
        "                                    top_p=0.95,\n",
        "                                    num_return_sequences=1\n",
        "                                )\n",
        "            for i, sample_output in enumerate(sample_outputs):\n",
        "                  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
        "\n",
        "            model.train()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_labels = batch[0].to(device)\n",
        "        b_masks = batch[1].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs  = model(b_input_ids,\n",
        "#                            token_type_ids=None,\n",
        "                             attention_mask = b_masks,\n",
        "                            labels=b_labels)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "        total_eval_loss += batch_loss\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "metadata": {
        "id": "jKUIZeYAXPQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voyons le résumé du processus de formation."
      ],
      "metadata": {
        "id": "io75SkzCXX-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher les flottants avec deux décimales.\n",
        "pd.set_option('display.precision', 2)\n",
        "\n",
        "# Créer un DataFrame à partir de nos statistiques d'entraînement.\n",
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "\n",
        "# Utiliser 'epoch' comme index de ligne.\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "# Un hack pour forcer les en-têtes de colonnes à s'enrouler.\n",
        "#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "\n",
        "# Affiche le tableau.\n",
        "df_stats"
      ],
      "metadata": {
        "id": "T1U6Z6I8XcPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utiliser le style d'intrigue de seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Augmenter la taille du tracé et la taille de la police.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Tracer la courbe d'apprentissage.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Entrainement\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Etiqueter le graphique.\n",
        "plt.title(\"Entrainement et Validation\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UgmJtUIXXiLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Afficher les informations sur le modèle"
      ],
      "metadata": {
        "id": "RNlBOjX1XpHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenir tous les paramètres du modèle sous la forme d'une liste de tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:2]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[2:14]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-2:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "id": "qdNhFGeJXqzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sauvegarde et chargement d'un modèle affiné"
      ],
      "metadata": {
        "id": "TVrrlO_ZX1_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarde des meilleures pratiques : si vous utilisez des noms par défaut pour le modèle, vous pouvez le recharger en utilisant from_pretrained().\n",
        "\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Créer un répertoire de sortie si nécessaire\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Sauvegarder un modèle entraîné, une configuration et un tokenizer en utilisant `save_pretrained()`.\n",
        "# Ils peuvent ensuite être rechargés en utilisant `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Prendre en charge la formation distribuée/parallèle\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Bonne pratique : sauvegarder les arguments d'entraînement avec le modèle entraîné\n",
        "# torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "metadata": {
        "id": "pQnLnm-3X4fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l --block-size=K ./model_save/"
      ],
      "metadata": {
        "id": "X_Q7kdXiYA1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l --block-size=M ./model_save/pytorch_model.bin"
      ],
      "metadata": {
        "id": "i88qjvqRYCLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copiez les fichiers du modèle dans un répertoire de votre Google Drive.\n",
        "!cp -r ./model_save/ $data_dir\n",
        "\n",
        "# # Charger un modèle entraîné et un vocabulaire que vous avez affiné\n",
        "#model = GPT2LMHeadModel.from_pretrained(output_dir)\n",
        "#tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n",
        "#model.to(device)"
      ],
      "metadata": {
        "id": "s3p-s5tUYGvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation de texte"
      ],
      "metadata": {
        "id": "0CsJ5DabYSBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "prompt = \"covid19 symptoms\"\n",
        "\n",
        "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "generated = generated.to(device)\n",
        "\n",
        "print(generated)\n",
        "\n",
        "sample_outputs = model.generate(\n",
        "                                generated,\n",
        "                                #bos_token_id=random.randint(1,30000),\n",
        "                                do_sample=True,\n",
        "                                top_k=50,\n",
        "                                max_length = 300,\n",
        "                                top_p=0.95,\n",
        "                                num_return_sequences=3\n",
        "                                )\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "PwxVjaSBYhJh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Bienvenue dans Colaboratory",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}